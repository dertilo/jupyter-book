{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "there are different types of policy-based reinforcement-learning\n",
    "![equations](images/RL_equations.png)\n",
    "[PyTorchA2CPolicy](https://gitlab.tu-berlin.de/OKS/plato/blob/actorcritic/DialogueManagement/DialoguePolicy/ReinforcementLearning/pytorch_a2c_policy.py#L78) implements the generalized A2C-Algorithm; A2C = Advantage Actor Critic  \n",
    "The NeuralNetwork is trained with a Loss that is comprised of three parts: \n",
    "1. policy-loss (for actor)\n",
    "2. entropy-regularization\n",
    "3. value-loss (for critic)\n",
    "\n",
    "it is trained in a __off-policy__ manner. Means, that losses are calculated with experience (dialogues) which where generated by an __outdate/old__ policy, this experience is __off-policy__. Hopefully not too far __off__. \n",
    "The __value-loss__ is the mean-squared-error of the __state-value__-estimate in step __k__ and the __overall-value__ in step __k+1__. In code this overall-value is called __returnn__. It is actually itself just an estimation and calculated by the state-value + advantage in step __k+1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(exps: Rollout, agent: AbstractA2CAgent, p: A2CParams):\n",
    "    dist, value = agent.calc_distr_value(exps.env_steps.observation)\n",
    "    entropy = dist.entropy().mean()\n",
    "    policy_loss = -(dist.log_prob(**exps.agent_steps.actions) * exps.advantages).mean()\n",
    "    value_loss = (value - exps.returnn).pow(2).mean()\n",
    "    loss = policy_loss - p.entropy_coef * entropy + p.value_loss_coef * value_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture\n",
    "The Neural Network consists of: \n",
    "    1. encoder\n",
    "    2. actor-head\n",
    "    3. critic-head\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyA2CAgent(AbstractA2CAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_intents,\n",
    "        num_slots,\n",
    "        encode_dim=64,\n",
    "        embed_dim=32,\n",
    "        padding_idx=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = StateEncoder(vocab_size, encode_dim, embed_dim, padding_idx)\n",
    "        self.actor = Actor(encode_dim, num_intents, num_slots)\n",
    "        self.critic = nn.Linear(encode_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features_pooled = self.encoder(x)\n",
    "        intent_probs, slots_sigms = self.actor(features_pooled)\n",
    "        value = self.critic(features_pooled)\n",
    "        return (intent_probs, slots_sigms), value\n",
    "\n",
    "    def calc_value(self, x):\n",
    "        value = self.critic(self.encoder(x))\n",
    "        return value\n",
    "\n",
    "    def calc_distr_value(self, state):\n",
    "        (intent_probs, slot_sigms), value = self.forward(state)\n",
    "        distr = CommonDistribution(intent_probs, slot_sigms)\n",
    "        return distr, value\n",
    "\n",
    "    def calc_distr(self, state):\n",
    "        distr, value = self.calc_distr_value(state)\n",
    "        return distr\n",
    "\n",
    "    def step(self, x) -> AgentStep:\n",
    "        (intent_probs, slot_sigms), value = self.forward(x)\n",
    "        distr = CommonDistribution(intent_probs, slot_sigms)\n",
    "        intent, slots = distr.sample()\n",
    "        v_values = value.data\n",
    "        return AgentStep((intent.item(), slots.numpy()), v_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generalized Advantage Calculation\n",
    "### Advantage\n",
    "\"how much better is a certain action towards the average\". in other words: \"What is the surplus of chosing this action in comparance to all other actions\". The average value  _V_ of a state is an action indepentend estimation by the critic. \n",
    "![advantage](images/advantage.jpg)\n",
    "\n",
    "### \"generalization\" \n",
    "![generalized](images/generalized_a2c_equation.jpg)\n",
    "in practice this sum does not go to infinity but some steps (e.g. 5) \"into the future\". The generalization is a discounted sum over future advantages. In the code these are called __bellman_delta__\n",
    "\n",
    "details see [berkeley-lecture-notes](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf#page=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_advantage_estimation(\n",
    "    rewards, values, dones, num_rollout_steps, discount, gae_lambda\n",
    "):\n",
    "    assert values.shape[0] == 1 + num_rollout_steps\n",
    "    advantage_buffer = torch.zeros(rewards.shape[0] - 1, rewards.shape[1])\n",
    "    next_advantage = 0\n",
    "    for i in reversed(range(num_rollout_steps)):\n",
    "        mask = torch.tensor((1 - dones[i + 1]), dtype=torch.float32)\n",
    "        bellman_delta = rewards[i + 1] + discount * values[i + 1] * mask - values[i]\n",
    "        advantage_buffer[i] = (\n",
    "            bellman_delta + discount * gae_lambda * next_advantage * mask\n",
    "        )\n",
    "        next_advantage = advantage_buffer[i]\n",
    "    return advantage_buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the [process_dialogue_to_turns](https://gitlab.tu-berlin.de/OKS/plato/blob/actorcritic/DialogueManagement/DialoguePolicy/ReinforcementLearning/pytorch_a2c_policy.py#L148) method the turns with acts that have not been created by the policy have to be filtered out. \n",
    "the __discounted_returns__ are currently not used, actually I wanted to substitude them with the rewards. But seems also to work like this. \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    def process_dialogue_to_turns(self, dialogue: List[Dict]) -> List[DialogTurn]:\n",
    "        assert dialogue[0][\"action\"][0].intent == \"welcomemsg\"\n",
    "        assert dialogue[-1][\"action\"][0].intent == \"bye\"\n",
    "        dialogue[-2][\"reward\"] = dialogue[-1][\"reward\"]\n",
    "        dialogue = dialogue[1:-1]\n",
    "        rewards = [t[\"reward\"] for t in dialogue]\n",
    "        returns = calc_discounted_returns(rewards, self.gamma)\n",
    "        turns = [\n",
    "            DialogTurn(\n",
    "                d[\"action\"][0],\n",
    "                tokenize(self.text_field, d[\"state\"]),\n",
    "                d[\"reward\"],\n",
    "                d[\"state\"].value,\n",
    "            )\n",
    "            for d, ret in zip(dialogue, returns)\n",
    "            if hasattr(d[\"state\"], \"value\")\n",
    "        ]\n",
    "        return turns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Encoding\n",
    "1. convert state to json\n",
    "2. tokenize and map to sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_json(state:SlotFillingDialogueState)->str:\n",
    "    temp = deepcopy(state)\n",
    "    del temp.context\n",
    "    del temp.system_requestable_slot_entropies\n",
    "    del temp.db_result\n",
    "    del temp.dialogStateUuid\n",
    "    del temp.user_goal\n",
    "    del temp.slots\n",
    "    del temp.item_in_focus\n",
    "    temp.db_matches_ratio = int(round(temp.db_matches_ratio, 2) * 100)\n",
    "    temp.slots_filled = [s for s,v in temp.slots_filled.items() if v is not None]\n",
    "    if temp.last_sys_acts is not None:\n",
    "        temp.last_sys_acts = action_to_string(temp.last_sys_acts, system=True)\n",
    "        temp.user_acts = action_to_string(temp.user_acts, system=False)\n",
    "\n",
    "    d = todict(temp)\n",
    "    assert d is not None\n",
    "    # d['item_in_focus'] = [(k,d['item_in_focus'] is not None and d['item_in_focus'].get(k,None) is not None) for k in self.domain.requestable_slots]\n",
    "    s = json.dumps(d)\n",
    "    # state_enc = int(hashlib.sha1(s.encode('utf-8')).hexdigest(), 32)\n",
    "    return s\n",
    "\n",
    "def tokenize(text_field, state: SlotFillingDialogueState):\n",
    "    state_string = state_to_json(state)\n",
    "    example = Example.fromlist([state_string], [(\"dialog_state\", text_field)])\n",
    "    tokens = [t for t in example.dialog_state if t in text_field.vocab.stoi]\n",
    "    return tokens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-web] *",
   "language": "python",
   "name": "conda-env-.conda-web-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
